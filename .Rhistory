dataframe
dataframe <- readLines(fileList[1])
dataframe
##Process each file
computeFile <- function(fileName){
dataframe <- readLines(fileName)
##remove empty lines
dataframe <- removeEmptyElements(dataframe);
##TODO remove comments
##TODO remove tabs
##Count complexity factors
factors <- c("if","else","case","default","return","for","while","do-while","break","continue","&&","\\|", "\\?", ":","catch", "finally", "throw", "throws");
complexity = 1;
for(line in dataframe){
line <- sapply(line, tolower);
print(line);
countVector <- str_count(line,factors);
complexity = complexity + sum(countVector);
}
#Load scripts
baseDir<- "C://Users//chris//OneDrive//Documentos//GitHub//";
dir<- paste(baseDir, "dataWrangling//dataframeUtil.R",sep="");
source(dir);
dirData <-paste(baseDir, "photinus-analytics//codeSnippetAnalysis//data//",sep="");
fileList <- list.files(dirData, full.names = TRUE)
dataframe <- readLines(fileList[1])
for(fileName in fileList){
computeFile(fileName);
}
;
;
#Load scripts
baseDir<- "C://Users//chris//OneDrive//Documentos//GitHub//";
dir<- paste(baseDir, "dataWrangling//dataframeUtil.R",sep="");
source(dir);
dirData <-paste(baseDir, "photinus-analytics//codeSnippetAnalysis//data//",sep="");
fileList <- list.files(dirData, full.names = TRUE);
dataframe <- readLines(fileList[1]);
for(fileName in fileList){
computeFile(fileName);
}
complexity
line <- dataframe[1]
line
line
line <- dataframe[2]
line
factors <- c("if","else","case","default","return","for","while","do-while","break","continue","&&","\\|", "\\?", ":","catch", "finally", "throw", "throws");
str_count(line,factors)
str_count(dataframe[3],factors)
str_count(dataframe[4],factors)
str_count(dataframe[4],factors)
str_count(dataframe[5],factors)
line <- dataframe[4]
line
str_count(dataframe[6],factors)
str_count(dataframe[7],factors)
factors <- c("if","else","case","default","return","for","while","do-while","break","continue","&&","\\|\\|", "\\?", ":","catch", "finally", "throw", "throws");
str_count(dataframe[7],factors)
dataframe[7]
str_count(dataframe[7],factors)
print(line)
##Process each file
computeFile <- function(fileName){
dataframe <- readLines(fileName)
##remove empty lines
dataframe <- removeEmptyElements(dataframe);
##TODO remove comments
##TODO remove tabs
##Count complexity factors
factors <- c("if","else","case","default","return","for","while","do-while","break","continue","&&","\\|\\|", "\\?", ":","catch", "finally", "throw", "throws");
complexity = 1;
for(line in dataframe){
line <- sapply(line, tolower);
print(line);
countVector <- str_count(line,factors);
complexity = complexity + sum(countVector);
}
}
fileList <- list.files(dirData, full.names = TRUE);
dataframe <- readLines(fileList[1]);
for(fileName in fileList){
computeFile(fileName);
}
complexity
##Process each file
computeFile <- function(fileName){
dataframe <- readLines(fileName)
##remove empty lines
dataframe <- removeEmptyElements(dataframe);
##TODO remove comments
##TODO remove tabs
##Count complexity factors
factors <- c("if","else","case","default","return","for","while","do-while","break","continue","&&","\\|\\|", "\\?", ":","catch", "finally", "throw", "throws");
complexity = 1;
for(line in dataframe){
line <- sapply(line, tolower);
print(line);
countVector <- str_count(line,factors);
print(sum(countVector));
complexity = complexity + sum(countVector);
}
}
fileList <- list.files(dirData, full.names = TRUE);
dataframe <- readLines(fileList[1]);
for(fileName in fileList){
computeFile(fileName);
}
?print
computeFile <- function(fileName){
dataframe <- readLines(fileName)
##remove empty lines
dataframe <- removeEmptyElements(dataframe);
##TODO remove comments
##TODO remove tabs
##Count complexity factors
factors <- c("if","else","case","default","return","for","while","do-while","break","continue","&&","\\|\\|", "\\?", ":","catch", "finally", "throw", "throws");
complexity = 1;
for(line in dataframe){
line <- sapply(line, tolower);
print(line);
countVector <- str_count(line,factors);
print(sum(countVector));
complexity = complexity + sum(countVector);
}
print(complexity);
}
fileList <- list.files(dirData, full.names = TRUE);
dataframe <- readLines(fileList[1]);
for(fileName in fileList){
computeFile(fileName);
}
complexity
computeFile <- function(fileName){
dataframe <- readLines(fileName)
##remove empty lines
dataframe <- removeEmptyElements(dataframe);
##TODO remove comments
##TODO remove tabs
##Count complexity factors
factors <- c("if","else","case","default","return","for","while","do-while","break","continue","&&","\\|\\|", "\\?", ":","catch", "finally", "throw", "throws");
complexity = 1;
for(line in dataframe){
line <- sapply(line, tolower);
print(line);
countVector <- str_count(line,factors);
print(sum(countVector));
complexity = complexity + sum(countVector);
}
return (complexity);
}
##Process each file
computeFile <- function(fileName){
dataframe <- readLines(fileName)
##remove empty lines
dataframe <- removeEmptyElements(dataframe);
##TODO remove comments
##TODO remove tabs
##Count complexity factors
factors <- c("if","else","case","default","return","for","while","do-while","break","continue","&&","\\|\\|", "\\?", ":","catch", "finally", "throw", "throws");
complexity = 1;
for(line in dataframe){
line <- sapply(line, tolower);
print(line);
countVector <- str_count(line,factors);
print(sum(countVector));
complexity = complexity + sum(countVector);
}
return (complexity);
}
#Load scripts
baseDir<- "C://Users//chris//OneDrive//Documentos//GitHub//";
dir<- paste(baseDir, "dataWrangling//dataframeUtil.R",sep="");
source(dir);
dirData <-paste(baseDir, "photinus-analytics//codeSnippetAnalysis//data//",sep="");
fileList <- list.files(dirData, full.names = TRUE);
dataframe <- readLines(fileList[1]);
for(fileName in fileList){
print("complexity:"+computeFile(fileName));
}
dir<- paste(baseDir, "dataWrangling//dataframeUtil.R",sep="");
source(dir);
dirData <-paste(baseDir, "photinus-analytics//codeSnippetAnalysis//data//",sep="");
fileList <- list.files(dirData, full.names = TRUE);
dataframe <- readLines(fileList[1]);
for(fileName in fileList){
print("complexity:",computeFile(fileName));
}
baseDir<- "C://Users//chris//OneDrive//Documentos//GitHub//";
dir<- paste(baseDir, "dataWrangling//dataframeUtil.R",sep="");
source(dir);
dirData <-paste(baseDir, "photinus-analytics//codeSnippetAnalysis//data//",sep="");
fileList <- list.files(dirData, full.names = TRUE);
dataframe <- readLines(fileList[1]);
for(fileName in fileList){
cat("complexity:",computeFile(fileName));
}
?qnorm
qnorm(0.95,0,1,lower.tail = FALSE, log.p = FALSE)
qnorm(95,0,1,lower.tail = FALSE, log.p = FALSE)
qnorm(0.95,0,1,lower.tail = TRUE, log.p = FALSE)
qnorm(0.975,0,1,lower.tail = TRUE, log.p = FALSE)
?dbeta
theta=seq(from=0,to=1,by=.01)
plot(theta,dbeta(theta,4,0),type="l")
plot(theta,dbeta(theta,4,1),type="l")
plot(theta,dbeta(theta,1,1),type="l")
theta=seq(from=0,to=1,by=.01)
plot(theta,dbeta(theta,4,2),type="l")
plot(theta,dbeta(theta,0,4),type="l")
?dbeta
plot(theta,dbeta(theta,1,4),type="l")
plot(theta,dbeta(theta,1,5),type="l")
1-pbeta(.25,41,11)
1-pbeta(.5,41,11)
1-pbeta(.8,41,11)
1-pbeta(.5,1,5)
pbeta(.5,1,5)
plot(theta,dbeta(theta,41,11),type="l")
1-pbeta(.5,41,11)
1-pbeta(.8,41,11)
?pbeta
1-pbeta(.8,41,11,lower.tail = TRUE)
1-pbeta(.8,1,5,lower.tail = TRUE)
theta=seq(from=0,to=1,by=.01)
plot(theta,dbeta(theta,1,5),type="l")
pbeta(.5,1,5)
pbeta(.5,1,5)
1- pbeta(.95,8,16)
1- pbeta(0.95,16,8)
qbeta(0.025,8,16)
qbeta(0.975,8,16)
1-pbeta(.8,41,11)
1-pbeta(.5,41,11)
1-pbeta(.25,41,11)
qbeta(.025,41,11)
qbeta(.975,41,11)
qbeta(0.05,8,16)
qbeta(0.95,8,16)
qbeta(0.90,8,16)
?pbeta
pgamma
pgamma
?pgamma
theta=seq(from=0,to=1,by=.01)
plot(theta,gamma(theta,67,6),type="l")
plot(theta,dgamma(theta,67,6),type="l")
plot(theta,dgamma(theta,8,1),type=".")
plot(theta,dgamma(theta,67,6),type="l")
plot(theta,dgamma(theta,8,1),type="-")
plot(theta,dgamma(theta,8,1),type="-")
lines(theta,dgamma(theta,67,6),lty=2)
lines(theta,dgamma(theta,8,1),lty=3
plot(theta,dbeta(theta,32,20),type="l")
lines(theta,dbeta(theta,8,4),lty=2)
lines(theta,44*dbinom(24,size=40,p=theta),lty=3)
plot(theta,dbeta(theta,32,20),type="l")
lines(theta,dbeta(theta,8,4),lty=2)
source('C:/Users/chris/OneDrive/Documentos/GitHub/bayesianStudies/CredibleIntervals.R', echo=TRUE)
plot(theta,dgamma(theta,8,1),type="l")
lines(theta,dgamma(theta,8,1),lty=3)
lines(theta,dgamma(theta,67,6),lty=2)
gam=seq(from=0,to=20,by=1)
plot(theta,dgamma(gam,8,1),type="l")
lines(theta,dgamma(gam,8,1),lty=3)
lines(theta,dgamma(gam,67,6),lty=2)
plot(theta,dgamma(gam,8,1),type="l")
gam=seq(from=0,to=20,by=1)
plot(theta,dgamma(gam,8,1),type="l")
plot(gam,dgamma(gam,8,1),type="l")
lines(gam,dgamma(gam,8,1),lty=3)
lines(gam,dgamma(gam,67,6),lty=2)
plot(gam,dgamma(gam,67,6),type="l")
lines(gam,dgamma(gam,8,1),lty=3)
lines(gam,dgamma(gam,67,6),lty=2)
plot(gam,dgamma(gam,67,6),type="l")
lines(gam,dgamma(gam,8,1),lty=2)
lines(gam,dgamma(gam,67,6),lty=3)
plot(gam,dgamma(gam,67,6),type="l")
lines(gam,dgamma(gam,8,1),lty=2)
qgamma(0.05,67,6)
qgamma(0.95,67,6)
install.packages('randomForest')
library(randomForest)
install.packages("rpart.plot")
install.packages('rattle')
install.packages('rpart.plot')
install.packages('RColorBrewer')
library(rpart)
library(RColorBrewer)
library(rattle)
library(rpart.plot)
source("C://Users//chris//OneDrive//Documentos//GitHub//randomForestWorkerConfidenceDifficulty//loadAnswers.R");
##
# Import data
dataf <- loadAnswers("answerList_data.csv");
summary(dataf$Answer.confidence)
#Remove NO AND IDK ANSWERS
dataf <- dataf [!(dataf$Answer.option=="IDK") ,];
dataf <- dataf [!(dataf$Answer.option=="NO") ,];
summary(dataf$Answer.option);
dataf$Worker.profession = as.factor(dataf$Worker.profession)
dataf$Worker.gender = as.factor(dataf$Worker.gender)
dataf$Worker.country = as.factor(dataf$Worker.country)
dataf$Answer.confidence = as.factor(dataf$Answer.confidence)
summary(dataf$Answer.option)
#Shuffle randomly the rows in the dataset
set.seed(9850)
g<- runif((nrow(dataf))) #generates a random distribution
dataf.rows <- dataf[order(g),]
model2 = randomForest(Answer.confidence ~
Code.complexity+
Code.LOC,
data = dataf.rows[1:458,], importance=TRUE, ntree=2000, type="prob");
test<-dataf[459:654,];
Prediction <- predict(model2, test);
submit <- data.frame(AnswerID = test$Answer.ID, PredictedConfidence = Prediction, Actual = test$Answer.confidence);
write.csv(submit, file = "firstforest.csv", row.names = FALSE);
model1$predicted
model2$predicted
model2$confusion
source("C://Users//chris//OneDrive//Documentos//GitHub//randomForestWorkerConfidenceDifficulty//loadAnswers.R");
##
# Import data
dataf <- loadAnswers("answerList_data.csv");
summary(dataf$Answer.confidence)
#Remove NO AND IDK ANSWERS
dataf <- dataf [!(dataf$Answer.option=="IDK") ,];
dataf <- dataf [!(dataf$Answer.option=="YES") ,];
summary(dataf$Answer.option);
dataf$Worker.profession = as.factor(dataf$Worker.profession)
dataf$Worker.gender = as.factor(dataf$Worker.gender)
dataf$Worker.country = as.factor(dataf$Worker.country)
dataf$Answer.confidence = as.factor(dataf$Answer.confidence)
summary(dataf$Answer.option)
#Shuffle randomly the rows in the dataset
set.seed(9850)
g<- runif((nrow(dataf))) #generates a random distribution
dataf.rows <- dataf[order(g),]
model2 = randomForest(Answer.confidence ~
Code.complexity+
Code.LOC,
data = dataf.rows[1:458,], importance=TRUE, ntree=2000, type="prob");
model2$confusion
model2 = randomForest(Answer.difficulty ~
Code.complexity+
Code.LOC,
data = dataf.rows[1:458,], importance=TRUE, ntree=2000, type="prob");
model2$confusion
dataf$Answer.difficulty = as.factor(dataf$Answer.difficulty)
model2 = randomForest(Answer.difficulty ~
Code.complexity+
Code.LOC,
data = dataf.rows[1:458,], importance=TRUE, ntree=2000, type="prob");
model2$confusion
model2 = randomForest(Answer.confidence ~
Code.complexity+
Code.LOC,
data = dataf.rows[1:458,], importance=TRUE, ntree=2000, type="prob");
model2$confusion
length(Answer.difficulty)
length(Answer.difficulty[,])
length(dataf$Answer.difficulty[,])
length(dataf$Answer.difficulty)
?round
totalData = length(dataf$Answer.difficulty);
trainingSize = trunc(totalData * 0.7);
testSize = totalData - trainingSize;
model2 = randomForest(Answer.difficulty ~
Code.complexity+
Code.LOC,
data = dataf.rows[1:trainingSize,], importance=TRUE, ntree=2000, type="prob");
dataf <- loadAnswers("answerList_data.csv");
summary(dataf$Answer.confidence)
#Remove NO AND IDK ANSWERS
#dataf <- dataf [!(dataf$Answer.option=="IDK") ,];
#dataf <- dataf [!(dataf$Answer.option=="YES") ,];
summary(dataf$Answer.option);
dataf$Worker.profession = as.factor(dataf$Worker.profession)
dataf$Worker.gender = as.factor(dataf$Worker.gender)
dataf$Worker.country = as.factor(dataf$Worker.country)
dataf$Answer.confidence = as.factor(dataf$Answer.confidence)
dataf$Answer.difficulty = as.factor(dataf$Answer.difficulty)
summary(dataf$Answer.option)
#Shuffle randomly the rows in the dataset
set.seed(9850)
g<- runif((nrow(dataf))) #generates a random distribution
dataf.rows <- dataf[order(g),]
totalData = length(dataf$Answer.difficulty);
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
endTestIndex = totalData;
model2 = randomForest(Answer.difficulty ~
Code.complexity+
Code.LOC,
data = dataf.rows[1:trainingSize,], importance=TRUE, ntree=2000, type="prob");
test<-dataf[startTestIndex:endTestIndex,];
Prediction <- predict(model2, test);
model2$confusion
dataf <- dataf [!(dataf$Answer.option=="IDK") ,];
dataf <- dataf [!(dataf$Answer.option=="YES") ,];
summary(dataf$Answer.option);
##
# Train model
# Treat worker profession as factor (categorical data)
dataf$Worker.profession = as.factor(dataf$Worker.profession)
dataf$Worker.gender = as.factor(dataf$Worker.gender)
dataf$Worker.country = as.factor(dataf$Worker.country)
dataf$Answer.confidence = as.factor(dataf$Answer.confidence)
dataf$Answer.difficulty = as.factor(dataf$Answer.difficulty)
summary(dataf$Answer.option)
#Shuffle randomly the rows in the dataset
set.seed(9850)
g<- runif((nrow(dataf))) #generates a random distribution
dataf.rows <- dataf[order(g),]
totalData = length(dataf$Answer.difficulty);
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
endTestIndex = totalData;
model2 = randomForest(Answer.difficulty ~
Code.complexity+
Code.LOC,
data = dataf.rows[1:trainingSize,], importance=TRUE, ntree=2000, type="prob");
test<-dataf[startTestIndex:endTestIndex,];
Prediction <- predict(model2, test);
model2$confusion
dataf <- loadAnswers("answerList_data.csv");
summary(dataf$Answer.confidence)
#Remove NO AND IDK ANSWERS
dataf <- dataf [!(dataf$Answer.option=="IDK") ,];
dataf <- dataf [!(dataf$Answer.option=="NO") ,];
summary(dataf$Answer.option);
dataf$Worker.profession = as.factor(dataf$Worker.profession)
dataf$Worker.gender = as.factor(dataf$Worker.gender)
dataf$Worker.country = as.factor(dataf$Worker.country)
dataf$Answer.confidence = as.factor(dataf$Answer.confidence)
dataf$Answer.difficulty = as.factor(dataf$Answer.difficulty)
summary(dataf$Answer.option)
#Shuffle randomly the rows in the dataset
set.seed(9850)
g<- runif((nrow(dataf))) #generates a random distribution
dataf.rows <- dataf[order(g),]
#FIRST TREE (loc,complexity,skill)
totalData = length(dataf$Answer.difficulty);
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
endTestIndex = totalData;
model2 = randomForest(Answer.difficulty ~
Code.complexity+
Code.LOC,
data = dataf.rows[1:trainingSize,], importance=TRUE, ntree=2000, type="prob");
test<-dataf[startTestIndex:endTestIndex,];
Prediction <- predict(model2, test);
model2$confusion
dataf <- loadAnswers("answerList_data.csv");
summary(dataf$Answer.confidence)
#Remove NO AND IDK ANSWERS
#dataf <- dataf [!(dataf$Answer.option=="IDK") ,];
#dataf <- dataf [!(dataf$Answer.option=="NO") ,];
summary(dataf$Answer.option);
##
# Train model
# Treat worker profession as factor (categorical data)
dataf$Worker.profession = as.factor(dataf$Worker.profession)
dataf$Worker.gender = as.factor(dataf$Worker.gender)
dataf$Worker.country = as.factor(dataf$Worker.country)
dataf$Answer.confidence = as.factor(dataf$Answer.confidence)
dataf$Answer.difficulty = as.factor(dataf$Answer.difficulty)
summary(dataf$Answer.option)
#Shuffle randomly the rows in the dataset
set.seed(9850)
g<- runif((nrow(dataf))) #generates a random distribution
dataf.rows <- dataf[order(g),]
#FIRST TREE (loc,complexity,skill)
#separates 70% of the data (1806 rows) to train and 30% (774) to test
#class means that we want a categorization
#the . means that all the other columns are features.
#Confidence prediction - YES Answers
totalData = length(dataf$Answer.difficulty);
trainingSize = trunc(totalData * 0.7);
startTestIndex = totalData - trainingSize;
endTestIndex = totalData;
model2 = randomForest(Answer.difficulty ~
Code.complexity+
Code.LOC,
data = dataf.rows[1:trainingSize,], importance=TRUE, ntree=2000, type="prob");
test<-dataf[startTestIndex:endTestIndex,];
Prediction <- predict(model2, test);
model2$confusion
model2$votes
model2$confusion
?predict;
Prediction2<- predict(model2.votes,test)
Prediction2<- predict(model2$votes,test)
Prediction
model2$ntree
model2$predicted
?randomForest()
Prediction <- predict(model2.rf, test,'vote');
Prediction <- predict(model2, test,'vote');
model2$confusion
model2$predicted
model2$votes
model2$predicted
submit <- data.frame(AnswerID = test$Answer.ID, PredictedConfidence = Prediction, Actual = test$Answer.confidence);
write.csv(submit, file = "firstforest.csv", row.names = FALSE);
test<-dataf[startTestIndex:endTestIndex,];
Prediction <- predict(model2, test,'vote');
submit <- data.frame(AnswerID = test$Answer.ID, PredictedConfidence = Prediction, Actual = test$Answer.confidence);
write.csv(submit, file = "firstforest.csv", row.names = FALSE);
submit <- data.frame(AnswerID = test$Answer.ID, PredictedLevel = Prediction, Actual = test$Answer.difficulty);
write.csv(submit, file = "firstforest.csv", row.names = FALSE);
